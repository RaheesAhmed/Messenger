import { BaseLanguageModelCallOptions } from "../base_language/index.js";
export interface OllamaInput {
    model?: string;
    baseUrl?: string;
    mirostat?: number;
    mirostatEta?: number;
    mirostatTau?: number;
    numCtx?: number;
    numGpu?: number;
    numThread?: number;
    repeatLastN?: number;
    repeatPenalty?: number;
    temperature?: number;
    stop?: string[];
    tfsZ?: number;
    topK?: number;
    topP?: number;
}
export interface OllamaRequestParams {
    model: string;
    prompt: string;
    options: {
        mirostat?: number;
        mirostat_eta?: number;
        mirostat_tau?: number;
        num_ctx?: number;
        num_gpu?: number;
        num_thread?: number;
        repeat_last_n?: number;
        repeat_penalty?: number;
        temperature?: number;
        stop?: string[];
        tfs_z?: number;
        top_k?: number;
        top_p?: number;
    };
}
export interface OllamaCallOptions extends BaseLanguageModelCallOptions {
}
export type OllamaGenerationChunk = {
    response: string;
    model: string;
    created_at: string;
    done: boolean;
};
export declare function createOllamaStream(baseUrl: string, params: OllamaRequestParams, options: OllamaCallOptions): AsyncGenerator<OllamaGenerationChunk>;
