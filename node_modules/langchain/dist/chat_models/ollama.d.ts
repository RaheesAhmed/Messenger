import { SimpleChatModel, BaseChatModelParams } from "./base.js";
import { BaseLanguageModelCallOptions } from "../base_language/index.js";
import { OllamaInput } from "../util/ollama.js";
import { CallbackManagerForLLMRun } from "../callbacks/manager.js";
import { BaseMessage, ChatGenerationChunk } from "../schema/index.js";
export interface OllamaCallOptions extends BaseLanguageModelCallOptions {
}
export declare class ChatOllama extends SimpleChatModel implements OllamaInput {
    CallOptions: OllamaCallOptions;
    lc_serializable: boolean;
    model: string;
    baseUrl: string;
    mirostat?: number;
    mirostatEta?: number;
    mirostatTau?: number;
    numCtx?: number;
    numGpu?: number;
    numThread?: number;
    repeatLastN?: number;
    repeatPenalty?: number;
    temperature?: number;
    stop?: string[];
    tfsZ?: number;
    topK?: number;
    topP?: number;
    constructor(fields: OllamaInput & BaseChatModelParams);
    _llmType(): string;
    invocationParams(options?: this["ParsedCallOptions"]): {
        model: string;
        options: {
            mirostat: number | undefined;
            mirostat_eta: number | undefined;
            mirostat_tau: number | undefined;
            num_ctx: number | undefined;
            num_gpu: number | undefined;
            num_thread: number | undefined;
            repeat_last_n: number | undefined;
            repeat_penalty: number | undefined;
            temperature: number | undefined;
            stop: string[] | undefined;
            tfs_z: number | undefined;
            top_k: number | undefined;
            top_p: number | undefined;
        };
    };
    _combineLLMOutput(): {};
    _streamResponseChunks(input: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
    protected _formatMessagesAsPrompt(messages: BaseMessage[]): string;
    /** @ignore */
    _call(messages: BaseMessage[], options: this["ParsedCallOptions"]): Promise<string>;
}
